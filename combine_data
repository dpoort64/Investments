# -*- coding: utf-8 -*-
"""
Created on Sat Sep 30 22:21:21 2017

@author: dpoor
"""

import bs4 as bs
import datetime as dt
import os 
import pandas as pd
import pandas_datareader as web
import pickle
import requests

#Function to save the S&P 500 Ticker symbols
#Will only run this function once as we save it to a "pickle" something or other
#Need to consider whether we should just save in a CSV file that updates daily or something
def save_sp500_tickers():
    resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
    soup = bs.BeautifulSoup(resp.text, 'lxml')
    table = soup.find('table', {'class':'wikitable sortable'})
    tickers = []
    
    for row in table.findAll('tr')[1:]:
        ticker = row.findAll('td')[0].text
        tickers.append(ticker)
    
    with open("sp500tickers.pickle", "wb") as f:
        pickle.dump(tickers, f)
    
    
    print(tickers)
    return tickers

#save_sp500_tickers()

# Get data from Yahoo for each company in S&P 500
# Probably want to start with only 10 or so just to make sure it is working
def get_data_from_yahoo(reload_sp500=False):
    if reload_sp500:
        tickers = save_sp500_tickers()
    else:
        #possibly could change this to be a csv file instead of pickle - not sure pros and cons
        with open("sp500tickers.pickle", "rb") as f:
            tickers = pickle.load(f)
    
    #Make the directory for stock quote CSV's if it does not already exist
    if not os.path.exists('stock_dfs'):
        os.makedirs('stock_dfs')
    
    #define variables for the looping through the stock list
    start = dt.datetime(2000,1,1)
    end = dt.datetime(2016,12,31)
    counter = 1
    flag = 0
    
    for ticker in tickers:
        exists_there = 0
        print("Start of for loop for counter #: ", counter, "and ticker symbol", ticker)
        
        if os.path.exists('stock_dfs/{}.csv'.format(ticker)):
            print('Already have {}'.format(ticker), " \n")    
            counter += 1
            exists_there += 1
        
        #Try for error (RemoteDataError) is common
        #But only if it did not already exist (to save time)    
        if exists_there == 0:
            print("Exists = :", exists_there)
            
            try:
                df = web.DataReader(ticker.replace('.','-' ), 'yahoo', start, end)
                df.to_csv('stock_dfs/{}.csv'.format(ticker))
            except Exception:
                print("Error in counter #: ", counter, "and ticker symbol", ticker)
                print("Flagging this entry")
                flag = 1
            
            #finally loop is always run in Try/Catch
            finally:
                print("I am inside first step of finally and flag = ", flag)
                
                #check to see if there is an issue with grabbing or writing the data
                #if no issue, then flag = 0, and proceed to write the csv file to the disk location                                                
                if flag == 0:
                    print("I am inside finally and flag = ", flag)

                    if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):
                        df = web.DataReader(ticker.replace('.', '-'), 'yahoo', start, end)
                        df.to_csv('stock_dfs/{}.csv'.format(ticker))
                        print("Ticker symbol ", ticker, " saved in counter #: ", counter, " sequence.")
                        counter += 1
                        print("Added new symbol and flag equalled", flag)
                    else:
                        print('Already have {}'.format(ticker))
                        counter+= 1
                        #break
                    
                #flag = 1 so we have some sort of error
                #so log the ticker symbol with the error into a csv file for further analysis (we may not care anyway, but I am learning)
                elif flag == 1:
                    print("File for ", ticker, " not created due to error")
                    counter += 1
                    flag = 0

get_data_from_yahoo()

#following code is start of next tutorial video from sentdex (Video 7 I think)
#commented for now
#the goal is to combine them all together
#we could have done this all in the function where we got the files, but better to save them online
#get adjusted close for all in one place
                    

def compile_data():
    with open("sp500tickers.pickle", "rb") as f:
        tickers=pickle.load(f)
    
    main_df = pd.DataFrame()

#iterate through the tickers file
    
    for count, ticker in enumerate(tickers): #enumerate lets us count things
        print('Counter #: ', count)
        #break
        try:
            df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))
            print('stock_dfs/{}.csv'.format(ticker) + ' found!')
            df.set_index('Date', inplace=True)
         
            df.rename(columns = {'Adj Close' : ticker}, inplace=True)
            df.drop(['Open','High','Low','Close','Volume'], 1, inplace=True)
         
            if main_df.empty:
                print('Main DF is empty')
                main_df = df
            else:
                print('Joining main df :', count)
                main_df = main_df.join(df, how ='outer')
                
        except:
            print('stock_dfs/{}.csv'.format(ticker) + ' not found')
            #break
        
        if count % 10 == 0:
            print('did my math and counter = :', count)
            print(main_df.head())
            main_df.to_csv('sp500_joined_closes.csv')
        
compile_data()
