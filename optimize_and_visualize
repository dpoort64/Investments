# -*- coding: utf-8 -*-
"""
Created on Sat Sep 30 22:21:21 2017

@author: dpoor
"""

import bs4 as bs
import datetime as dt
import matplotlib.pyplot as plt
from matplotlib import style
import numpy as np
import os 
import pandas as pd
import pandas_datareader as web
import pickle
import requests


style.use('ggplot')
#Function to save the S&P 500 Ticker symbols
#Will only run this function once as we save it to a "pickle" something or other
#Need to consider whether we should just save in a CSV file that updates daily or something
def save_sp500_tickers():
    resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
    soup = bs.BeautifulSoup(resp.text, 'lxml')
    table = soup.find('table', {'class':'wikitable sortable'})
    tickers = []
    
    for row in table.findAll('tr')[1:]:
        ticker = row.findAll('td')[0].text
        tickers.append(ticker)
    
    with open("sp500tickers.pickle", "wb") as f:
        pickle.dump(tickers, f)
    
    
    print(tickers)
    return tickers

#save_sp500_tickers()

# Get data from Yahoo for each company in S&P 500
# Probably want to start with only 10 or so just to make sure it is working
# Had to add my own code for Try/Except to catch errors that were stopping the process
#After a lot of effort, went back to original, and made the code far more efficient
#No need for flags, multiple try/rxcepts, etc.
#still need to understand what to do with errors
    

def get_data_from_yahoo(reload_sp500=False):
    if reload_sp500:
        tickers = save_sp500_tickers()
    else:
        #possibly could change this to be a csv file instead of pickle - not sure pros and cons
        with open("sp500tickers.pickle", "rb") as f:
            tickers = pickle.load(f)
    
    #Make the directory for stock quote CSV's if it does not already exist
    if not os.path.exists('stock_dfs'):
        os.makedirs('stock_dfs')
    
    #define variables for the looping through the stock list
    start = dt.datetime(2000,1,1)
    end = dt.datetime(2016,12,31)
    counter = 1
    error_count = 0
    
    for ticker in tickers:
        try:
            if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):
                df = web.DataReader(ticker.replace('.', '-'), 'yahoo', start, end)
                df.to_csv('stock_dfs/{}.csv'.format(ticker))
                print("Ticker symbol ", ticker, " saved in counter #: ", counter, " sequence.")
                counter += 1
            else:
                print('Already found {}'.format(ticker))
        except:
             print("Error in counter #: ", counter, "and ticker symbol", ticker)
             error_count += 1
             print("Error count is: ", error_count)
             counter += 1      

#get_data_from_yahoo()

#following code is start of next tutorial video from sentdex (Video 7 I think)
#commented for now
#the goal is to combine them all together
#we could have done this all in the function where we got the files, but better to save them online
#get adjusted close for all in one place

def compile_data():
    with open("sp500tickers.pickle", "rb") as f:
        tickers=pickle.load(f)
    
    main_df = pd.DataFrame()
    
#iterate through the tickers file
    
    for count, ticker in enumerate(tickers): #enumerate lets us count things
        print('Counter #: ', count)
        #break
        try:
            df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))
            print('stock_dfs/{}.csv'.format(ticker) + ' found!')
            df.set_index('Date', inplace=True)
         
            df.rename(columns = {'Adj Close' : ticker}, inplace=True)
            df.drop(['Open','High','Low','Close','Volume'], 1, inplace=True)
         
            if main_df.empty:
                print('Main DF is empty')
                main_df = df
            else:
                print('Joining main df :', count)
                main_df = main_df.join(df, how ='outer')
                
        except:
            print('stock_dfs/{}.csv'.format(ticker) + ' not found')
            #break
        
        if count % 10 == 0:
            print('did my math and counter = :', count)
            print(main_df.head())
            main_df.to_csv('sp500_joined_closes.csv')
        
#compile_data()

#work with the data. can we find any relationships
#Relationship over 17 years
#some data is NAN

def visualize_data():
    df = pd.read_csv('sp500_joined_closes.csv')
    
    #df['AAPL'].plot()
    #plt.show()
    df_corr = df.corr()
    #print(df_corr.head())
    
    data = df_corr.values #show just the values with no headers/labels
    fig = plt.figure()
    ax = fig.add_subplot(111)
    
    heatmap = ax.pcolor(data, cmap=plt.cm.RdYlGn)
    fig.colorbar(heatmap)
    ax.set_xticks(np.arange(data.shape[0]) + 0.5, minor=False)
    ax.set_yticks(np.arange(data.shape[1]) + 0.5, minor=False)
    ax.invert_yaxis()
    ax.xaxis.tick_top()
    
    column_labels = df_corr.columns
    row_labels = df_corr.index
    
    ax.set_xticklabels(column_labels)
    ax.set_yticklabels(row_labels)
    plt.xticks(rotation=90)
    heatmap.set_clim(-1, 1)
    plt.tight_layout()
    plt.show()
    
#visualize_data()
