# -*- coding: utf-8 -*-
"""
Created on Sat Sep 30 22:21:21 2017

@author: dpoor
"""

import bs4 as bs
import datetime as dt
import os 
import pandas as pd
import pandas_datareader as web
import pickle
import requests
import csv

#Function to save the S&P 500 Ticker symbols
#Will only run this function once as we save it to a "pickle" something or other
#Need to consider whether we should just save in a CSV file that updates daily or something
def save_sp500_tickers():
    resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
    soup = bs.BeautifulSoup(resp.text, 'lxml')
    table = soup.find('table', {'class':'wikitable sortable'})
    tickers = []
    
    for row in table.findAll('tr')[1:]:
        ticker = row.findAll('td')[0].text
        tickers.append(ticker)
    
    with open("sp500tickers.pickle", "wb") as f:
        pickle.dump(tickers, f)
    
    print(tickers)
    
    return tickers

#save_sp500_tickers()

# Get data from Yahoo for each company in S&P 500
# Probably want to start with only 10 or so just to make sure it is working
def get_data_from_yahoo(reload_sp500=False):
    if reload_sp500:
        tickers = save_sp500_tickers()
    else:
        #possibly could change this to be a csv file instead of pickle - not sure pros and cons
        with open("sp500tickers.pickle", "rb") as f:
            tickers = pickle.load(f)
    
    #Make the directory for stock quote CSV's if it does not already exist
    if not os.path.exists('stock_dfs'):
        os.makedirs('stock_dfs')
    
    #define variables for the looping through the stock list
    start = dt.datetime(2010,1,1)
    end = dt.datetime(2017,9,30)
    counter = 1
    flag = 0
    
    for ticker in tickers:
        exists_there = 0
        print("Start of for loop for counter #: ", counter, "and ticker symbol", ticker)
        
        if os.path.exists('stock_dfs/{}.csv'.format(ticker)):
            print('Already have {}'.format(ticker), " \n")    
            counter += 1
            exists_there += 1
        
        #Try for error (RemoteDataError) is common
        #But only if it did not already exist (to save time)    
        if exists_there == 0:
            print("Exists = :", exists_there)
            
            try:
                df = web.DataReader(ticker, 'yahoo', start, end)
                df.to_csv('stock_dfs/{}.csv'.format(ticker))
            except Exception:
                print("Error in counter #: ", counter, "and ticker symbol", ticker)
                print("Flagging this entry")
                flag = 1
            
            #finally loop is always run in Try/Catch
            finally:
                print("I am inside first step of finally and flag = ", flag)
                
                #check to see if there is an issue with grabbing or writing the data
                #if no issue, then flag = 0, and proceed to write the csv file to the disk location                                                
                if flag == 0:
                    print("I am inside finally and flag = ", flag)

                    if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):
                        df = web.DataReader(ticker, 'yahoo', start, end)
                        df.to_csv('stock_dfs/{}.csv'.format(ticker))
                        print("Ticker symbol ", ticker, " saved in counter #: ", counter, " sequence.")
                        counter += 1
                        print("Added new symbol and flag equalled", flag)
                    else:
                        print('Already have {}'.format(ticker))
                        counter+= 1
                        #break
                    
                #flag = 1 so we have some sort of error
                #so log the ticker symbol with the error into a csv file for further analysis (we may not care anyway, but I am learning)
                elif flag == 1:
                    print("File for ", ticker, " not created due to error")
                    print("About to open the csv file.")
                    
                    bad_ticker_list = open("sp500missed.csv", "a")
                    writer = csv.writer(bad_ticker_list)
                    writer.writerow([ticker])
                    bad_ticker_list.close()
                    
                    counter += 1
                    flag = 0


get_data_from_yahoo()

#following code is start of next tutorial video from sentdex (Video 7 I think)
#commented for now

'''
def compile_data():
    with open("sp500tickers.pickle", "rb") as f:
        tickers=pickle.load(f)
    
    main_df = pd.DataFrame()
    
    for count, ticker in enumerate(tickers):
        df = pd.read_csv('stock_sfs/{}.csv'.format(ticker))
        df.set_index('Date', inplace=True)
        
        df.rename(columns = {'Adj Close', ticker}, inplace=True)
        ##stopped here for now - need to look into straight web scraping as learning
        ##video #7, minutes or so in
        
            
            
    
    
    
''' 
